# -*- coding: utf-8 -*-
"""Get_Index.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OFmSDN9FXoEAJP2hiQqMn7HSegDocvJ3
"""

# !pip install llama_index
# !pip install transformers accelerate bitsandbytes
# !pip install pypdf
# !pip install neo4j
#!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python

from llama_index import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    ServiceContext,
    download_loader,
    PromptHelper,
    StorageContext,
    load_index_from_storage,
    StorageContext,
    KnowledgeGraphIndex,
)
from llama_index.query_engine import KnowledgeGraphQueryEngine
from llama_index.graph_stores import Neo4jGraphStore
from llama_index.embeddings import HuggingFaceEmbedding, OpenAIEmbedding, LangchainEmbedding
from llama_index.response.notebook_utils import display_response
from pathlib import Path
from llama_index import download_loader
from llama_index.llms import LlamaCPP
from llama_index.text_splitter import SentenceSplitter


#---------------------------------------#
import torch
from transformers import BitsAndBytesConfig
from llama_index.prompts import PromptTemplate
from llama_index.callbacks import CallbackManager, LlamaDebugHandler
from llama_index.llms import HuggingFaceLLM
from llama_index.llms.llama_utils import (
    messages_to_prompt,
    completion_to_prompt,
)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)
import logging
import sys
import os

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

llama_debug = LlamaDebugHandler(print_trace_on_end=True)
callback_manager = CallbackManager([llama_debug])
model_url = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf?download=true"

# Change LLMs

 llm = LlamaCPP("......")

#------------------------Read file------------------#


PDFReader = download_loader("PDFReader")

loader = PDFReader()
documents = loader.load_data(file=Path('......'))

#--------------------------Define service_context----------------------------#

from llama_index.node_parser import SentenceSplitter
#embed_model = OpenAIEmbedding() #sentence-transformers/all-roberta-large-v1
embed_model = LangchainEmbedding(HuggingFaceEmbedding("sentence-transformers/all-roberta-large-v1")) #local:BAAI/bge-small-en-v1.5
service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size = 128, chunk_overlap = 120)

#---------------Set up database------------------#

username = "neo4j"
password = "7q0G7tP_ZxJGK-HH87pNoLMqcCO4sDN2cQnPIASwzLw"
url = "neo4j+s://39992655.databases.neo4j.io"
database = "neo4j"

graph_store = Neo4jGraphStore(
    username=username,
    password=password,
    url=url,
    database=database,
)

storage_context = StorageContext.from_defaults(graph_store=graph_store)

#-------------------Prompt-----------------------#
prompt = """
<s><INST> ## 1. Overview
You are a top-tier algorithm designed for extracting information about a guide for employees who want to understand labor laws in structured formats to build a knowledge graph.
- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.
- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.
## 2. Labeling Nodes
- **Consistency**: Ensure you use basic or elementary types for node labels.
  - For example, when you identify an entity representing a person, always label it as **"person"**. Avoid using more specific terms like "mathematician" or "scientist".
- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.
## 3. Handling Numerical Data and Dates
- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.
- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.
- **Property Format**: Properties must be in a key-value format.
- **Quotation Marks**: Never use escaped single or double quotes within property values.
- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.
## 4. Coreference Resolution
- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.
If an entity, such as "John Doe", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., "Joe", "he"),
always use the most complete identifier for that entity throughout the knowledge graph. In this example, use "John Doe" as the entity ID.
Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.
## 5. Strict Compliance
Adhere to the rules strictly.Do not contain any explanations or appologies in your response.Non-compliance will result in termination.

Example:
 "Some text is provided below. Given the text, extract up to "
    "{max_knowledge_triplets} "
    "knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\n"
    "---------------------\n"
    "Example:"
    "Text: Alice is Bob's mother."
    "Triplets:\n(Alice, is mother of, Bob)\n"
    "Text: Philz is a coffee shop founded in Berkeley in 1982.\n"
    "Triplets:\n"
    "(Philz, is, coffee shop)\n"
    "(Philz, founded in, Berkeley)\n"
    "(Philz, founded in, 1982)\n"
    "---------------------\n"
    "Text: {text}\n"
    "Triplets:\n"
 </INST>
"""
prompt_template = PromptTemplate(prompt, prompt_type = "knowledge_triplet_extract")

#------------------Get index---------------------#

index = KnowledgeGraphIndex.from_documents(
    doc,
    storage_context=storage_context,
    max_triplets_per_chunk=10, #10, 15, 20
    #kg_triplet_extract_fn=extract_triplets,
    service_context=service_context,
    kg_triple_extract_template=prompt_template,
    #include_embedding = True,
    show_progress = True
)

index.storage_context.persist(persist_dir = './content/graph_chatbot') # Save the index for future retrieval
